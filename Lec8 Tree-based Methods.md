# DDA3020 Machine Learning - Lecture 08: Tree-based Methods 知识点总结

## 1. 动机 (Motivation)

### 1.1 参数化模型 (Parametric Models)
在此之前学习的模型（线性回归、逻辑回归、SVM）多为参数化模型。
*   **定义**：
    *   **训练阶段**：在整个输入空间上定义一个假设函数（模型），并通过所有训练数据学习固定数量的参数。
    *   **测试阶段**：对任何测试输入使用相同的模型和参数集。
*   **局限性**：
    *   模型形式由训练者预先决定（如假设是线性的），可能偏离输入输出之间的真实关系（Ground-truth）。
    *   决策/预测过程难以解释，缺乏清晰的决策步骤（黑盒特性）。

### 1.2 非参数化模型 (Nonparametric Models)
*   **定义**：不依赖于关于变量关系形状的强假设。数据本身决定了拟合函数的形式。
*   **典型模型**：K-近邻 (KNN)、决策树 (Decision Tree)。
*   **决策树的优势**：
    *   **层级化**：一种层级化的非参数模型。
    *   **可解释性**：具有特殊的优势，其决策过程是分层的，易于人类理解。

---

## 2. 决策树基础 (Decision Tree Basics)

### 2.1 定义与术语
*   **定义**：一种用于监督学习的层级模型，通过一系列递归分割来识别局部区域。
*   **结构**：
    *   **根节点 (Root)**：包含所有训练样本。
    *   **内部节点 (Internal Node)**：代表一个属性上的测试。
    *   **分支 (Branch)**：代表测试的可能结果。
    *   **叶节点 (Leaf)**：代表最终的类标签（分类）或数值（回归）。
*   **类型**：
    *   **二叉树 (Binary Tree)**：每个节点最多两个子节点（如 CART 算法）。
    *   **多叉树 (Multi-way Tree)**：每个节点可有多个子节点（如 ID3 算法）。
*   **深度 (Depth)**：从根到叶的最长路径长度。
*   **大小 (Size)**：树中节点的总数。

### 2.2 树的学习算法 (Tree Learning Algorithm)
*   **策略**：贪心算法 (Greedy Algorithm)，自顶向下递归的分治策略 (Divide-and-conquer)。
*   **基本流程**：
    1.  开始时，所有训练样本都在根节点。
    2.  基于选定的属性递归地划分样本（连续值属性需预先离散化）。
    3.  基于启发式或统计度量（如信息增益）选择测试属性。
*   **停止条件**：
    *   所有叶节点是纯的（只包含一类）。
    *   达到最大深度。
    *   达到某种性能指标。
    *   **注意**：寻找最小的无误差树是 NP-complete 问题，因此必须使用启发式局部搜索。

---

## 3. 分类树 (Classification Tree)

### 3.1 属性选择度量 (Attribute Selection Measure)
在每一步选择哪个属性进行分割？目标是减少不纯度 (Impurity)。

#### 3.1.1 不纯度 (Impurity) 定义
设 $S$ 为到达某节点的训练实例集合，$|S|$ 为实例总数。
设 $S$ 中属于类 $C_i$ ($i=1, \dots, K$) 的实例集合为 $S_i$。
该节点属于类 $C_i$ 的概率估计为：
$$ p_i = P(C_i | x, S) = \frac{|S_i|}{|S|} $$
*   **纯节点 (Pure Node)**：存在某个 $i$ 使得 $p_i = 1$。此时无需继续分裂。

#### 3.1.2 熵 (Entropy / Information)
熵是信息论中衡量不确定性的指标，表示编码实例类别所需的最小比特数。

*   **多分类节点的熵公式**：
    $$ Info(S) = - \sum_{i=1}^K p_i \log_2 p_i $$
    *   当样本均匀分布在各类时，熵最大（最不纯）。
    *   当样本全属于同一类时，熵为 0（最纯）。

*   **二分类节点的熵公式** ($p$ 为正类概率)：
    $$ Info(S) = -p \log_2 p - (1-p) \log_2 (1-p) $$

#### 3.1.3 信息增益 (Information Gain)
衡量使用属性 $A$ 分割集合 $S$ 后，不确定性（熵）减少了多少。

1.  **条件熵 (Conditional Entropy)**：
    假设属性 $A$ 有 $V$ 个可能的值 $\{a_1, \dots, a_V\}$，将 $S$ 分割为 $V$ 个子集 $D_1, \dots, D_V$。
    $$ Info(S|A) = \sum_{v=1}^V \frac{|D_v|}{|S|} \times Info(D_v) $$
    *   解释：这是分割后各子集熵的加权平均，权重为子集大小占总大小的比例。

2.  **信息增益公式**：
    $$ Gain(A) = Info(S) - Info(S|A) $$
    *   **推导逻辑**：
        $$ \text{Gain} = \text{分割前的总不确定性} - \text{分割后的期望不确定性} $$
    *   **决策规则**：选择 $Gain(A)$ 最大的属性进行分割。

# 决策树：信息增益计算 (三分类实例)

## 1. 场景设定
这是一个多分类问题。
*   **目标标签 $S$**: 天气状况 (3类: Sunny, Cloudy, Rainy)
*   **分割属性 $A$**: 风力等级 (2类: Weak, Strong)
*   **总样本数**: 14

**数据分布统计：**
*   **整体 $S$ (14)**: Sunny=5, Cloudy=5, Rainy=4
*   **分支 $A=Weak$ (8)**: Sunny=4, Cloudy=3, Rainy=1
*   **分支 $A=Strong$ (6)**: Sunny=1, Cloudy=2, Rainy=3

---

## 2. 详细计算过程

### 第一步：计算分割前的总熵 $Info(S)$
由于是三分类，公式包含三项：
$$Info(S) = -\sum_{i=1}^{3} p_i \log_2 p_i$$

$$
\begin{aligned}
p(Sunny) &= 5/14 \approx 0.357 \\
p(Cloudy) &= 5/14 \approx 0.357 \\
p(Rainy) &= 4/14 \approx 0.286 \\
\\
Info(S) &= -(\frac{5}{14}\log_2\frac{5}{14}) -(\frac{5}{14}\log_2\frac{5}{14}) -(\frac{4}{14}\log_2\frac{4}{14}) \\
&\approx 0.530 + 0.530 + 0.517 \\
&= \mathbf{1.577 \text{ bits}}
\end{aligned}
$$

### 第二步：计算分割后的条件熵 $Info(S|A)$
公式：$Info(S|A) = \frac{|D_{Weak}|}{|S|}Info(D_{Weak}) + \frac{|D_{Strong}|}{|S|}Info(D_{Strong})$

**1. 计算分支 "Weak" (弱风) 的熵:**
样本分布: [4, 3, 1], 总数 8
$$
\begin{aligned}
Info(Weak) &= -(\frac{4}{8}\log_2\frac{4}{8}) -(\frac{3}{8}\log_2\frac{3}{8}) -(\frac{1}{8}\log_2\frac{1}{8}) \\
&= 0.5 + 0.531 + 0.375 \\
&= \mathbf{1.406 \text{ bits}}
\end{aligned}
$$

**2. 计算分支 "Strong" (强风) 的熵:**
样本分布: [1, 2, 3], 总数 6
$$
\begin{aligned}
Info(Strong) &= -(\frac{1}{6}\log_2\frac{1}{6}) -(\frac{2}{6}\log_2\frac{2}{6}) -(\frac{3}{6}\log_2\frac{3}{6}) \\
&= 0.432 + 0.528 + 0.5 \\
&= \mathbf{1.460 \text{ bits}}
\end{aligned}
$$

**3. 计算加权平均 (条件熵):**
$$
\begin{aligned}
Info(S|Wind) &= \frac{8}{14}(1.406) + \frac{6}{14}(1.460) \\
&\approx 0.803 + 0.626 \\
&= \mathbf{1.429 \text{ bits}}
\end{aligned}
$$

### 第三步：计算信息增益 $Gain(A)$
公式：$Gain(A) = Info(S) - Info(S|A)$

$$
\begin{aligned}
Gain(Wind) &= 1.577 - 1.429 \\
&= \mathbf{0.148 \text{ bits}}
\end{aligned}
$$

## 3. 总结
*   在三分类问题中，熵的理论最大值为 $\log_2 3 \approx 1.585$。
*   本例中初始熵为 1.577（非常混乱）。
*   使用“风力”分割后，熵降至 1.429。
*   **信息增益为 0.148**。虽然有增益，但数值较小，说明仅靠“风力”很难准确区分这三种天气。
#### 3.1.4 基尼指数 (Gini Index)
另一种衡量不纯度的方法，常用于 CART 算法。

*   **基尼指数公式**：
    $$ Gini(S) = 1 - \sum_{i=1}^K p_i^2 $$
    *   解释：表示从集合中随机选取两个样本，它们属于不同类别的概率。

*   **属性 A 的期望基尼指数**（类似于条件熵）：
    $$ Gini(S|A) = \sum_{v=1}^V \frac{|D_v|}{|S|} Gini(D_v) $$

*   **基尼不纯度减少量**：
    $$ \Delta Gini(A) = Gini(S) - Gini(S|A) $$

*   **熵 vs. 基尼指数**：
    *   Gini 计算更简单（无需对数运算）。
    *   Gini 更具可解释性。
    *   熵在类别不平衡时更有效。
    *   熵对噪声较不敏感。

---

## 4. 回归树 (Regression Trees)

### 4.1 基本概念
*   **区别**：目标变量是连续值，不再使用熵或基尼指数，而是使用误差平方和 (SSE) 或均方误差 (MSE)。
*   **预测值**：叶节点 $c$ 的预测值 $\bar{y}_c$ 通常是该叶节点内所有样本目标值的均值。
    $$ \bar{y}_c = \frac{1}{N_c} \sum_{i \in c} y_i $$

### 4.2 损失函数与构建算法
*   **节点内的误差平方和 (SSE)**：
    $$ S_c = \sum_{i \in c} (y_i - \bar{y}_c)^2 $$
*   **整棵树的总 SSE**：
    $$ S_{total} = \sum_{c \in leaves(Tree)} S_c $$

*   **分裂准则 (Splitting Criterion)**：
    寻找一个切分点（例如变量 $x$ 的阈值 $w_1$），将节点分为左子节点 $c_L$ 和右子节点 $c_R$，使得分裂后的总 SSE 最小（即 SSE 下降最大）。

    1.  计算分裂前的 SSE：$S_{parent}$。
    2.  计算分裂后的 SSE ($S_{w1}$)：
        $$ S_{w1} = \underbrace{\sum_{i \in c_L} (y_i - \bar{y}_{c_L})^2}_{\text{左子节点SSE}} + \underbrace{\sum_{i \in c_R} (y_i - \bar{y}_{c_R})^2}_{\text{右子节点SSE}} $$
    3.  **优化目标**：找到 $w_1$ 最大化误差下降：
        $$ w_1^* = \arg\max_{w_1} (S_{parent} - S_{w1}) $$

---

## 5. 过拟合与剪枝 (Overfitting and Pruning)

### 5.1 过拟合现象
*   树倾向于过拟合训练数据，导致测试误差高。
*   表现为决策边界高度不平滑，模型过于复杂，记住了噪声。

### 5.2 剪枝 (Pruning)
通过剪枝简化模型，提高泛化能力。

*   **通用过程**：
    1.  将数据分为训练集 (Training set) 和验证集 (Validation set)。
    2.  在训练集上生长一棵深树（Overgrown tree）。
    3.  **贪心剪枝**：评估剪掉某个节点（将其替换为叶节点）对验证集准确率的影响。如果剪枝能提高或不降低验证集性能，则执行剪枝。

*   **减少误差剪枝 (Reduced-Error Pruning)**：
    *   操作：将整个子树替换为一个叶节点。
    *   规则：如果子树在验证集上的预期误差率高于单个叶节点，则替换。
    *   结果：树的大小减小，决策边界变平滑，测试集准确率通常提升。

---

## 6. 集成模型 (Ensemble Models)

### 6.1 核心思想
*   **单棵树的缺点**：单棵剪枝树预测能力弱，单棵深树易过拟合。
*   **集成**：构建多个差异化（Diverse）的决策树，组合它们的预测（分类用多数投票，回归用平均）。
*   **哲学**："群体的智慧" (Wisdom of the crowd)，三个臭皮匠胜过诸葛亮。

### 6.2 Bagging (Bootstrap Aggregating)
*   **机制**：
    1.  **Bootstrap 采样**：从原始数据中有放回地随机采样，生成多个不同的训练数据集。这引入了**数据层面的随机性**。
    2.  **模型训练**：在每个重采样的数据集上拟合一棵过生长的树（Overgrown tree）。
    3.  **聚合**：对所有单棵树的预测结果进行聚合（投票或平均）。
*   **效果**：随着树的数量增加，预测误差通常会下降。
*   **局限**：由于重采样数据间存在大量共享样本，生成的树可能高度相关 (Correlated)，导致多样性不足。

### 6.3 随机森林 (Random Forest)
*   **改进**：为了减少 Bagging 中树之间的相关性，引入了**属性层面的随机性**。
*   **算法流程**：
    1.  同 Bagging 进行 Bootstrap 数据采样。
    2.  在构建树的每一步分裂时，**不是**在所有 $N$ 个属性中搜索最佳分裂，而是限制在**随机选择的 $m$ 个属性子集**中搜索。
*   **参数 $m$ 的选择**：
    *   回归树：$m \approx \frac{N}{3}$
    *   分类树：$m \approx \sqrt{N}$
*   **性能对比**：
    $$ \text{Prediction Error: Random Forest} < \text{Bagging} < \text{Single Trees} $$
*   **解释**：集成模型通过让每棵树过拟合不同的数据子集和属性子集，相互抵消了偏差和方差，从而避免了对原始固定数据集的过拟合。

---

## 7. 总结：决策树优缺点

*   **优点**：
    *   易于理解和解释（可视化）。
    *   数据预处理要求低（不需要归一化，可处理缺失值）。
    *   能处理数值型和类别型数据。
    *   非参数方法，灵活性高。
*   **缺点**：
    *   **过拟合**：容易构建过于复杂的树。需通过剪枝或集成方法解决。
    *   **连续变量**：在处理连续变量时，离散化（量化）过程会丢失信息。
    *   **不稳定性**：数据的小变化可能导致生成完全不同的树（高方差）。